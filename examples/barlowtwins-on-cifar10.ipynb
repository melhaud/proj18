{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Example: Train Barlow Twins on CIFAR10\n",
    "<a href=\"https://colab.research.google.com/github/melhaud/proj18/blob/main/examples/barlowtwins-on-cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>\n",
    "\n",
    "In this tutorial, we will train a Barlow Twins model using lightly. The model,\n",
    "augmentations and training procedure is from \n",
    "`Barlow Twins: Self-Supervised Learning via Redundancy Reduction <https://arxiv.org/abs/2103.03230>`.\n",
    "\n",
    "The paper explores a rather simple training procedure for contrastive learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_135357/2693792829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import lightly\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from lightly.data import LightlyDataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('../src')\n",
    "from utils import custom_collate_fn, get_classes\n",
    "from my_resnet import resnet20\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Configuration parameters for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_135357/919013753.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Let's set the seed for our experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "num_workers = 2\n",
    "batch_size = 128\n",
    "seed = 1\n",
    "\n",
    "max_epochs = 150\n",
    "input_size = 32 # image height, assume its always square\n",
    "\n",
    "# Let's set the seed for our experiments\n",
    "\n",
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data augmentations and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train = CIFAR10(\"../data/cifar10\", download=True, train=True)\n",
    "cifar10_test = CIFAR10(\"../data/cifar10\", download=True, train=False)\n",
    "\n",
    "classes_ids_train = get_classes(cifar10_train) # long!\n",
    "classes_ids_test = get_classes(cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = lightly.data.SimCLRCollateFunction(\n",
    "    input_size=input_size,\n",
    "    vf_prob=0.5,\n",
    "    rr_prob=0.5\n",
    ")\n",
    "\n",
    "# We create a torchvision transformation for embedding the dataset after \n",
    "# training\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_train_simclr = LightlyDataset.from_torch_dataset(Subset(cifar10_train, classes_ids_train['dog']))\n",
    "\n",
    "dataloader_train_simclr = torch.utils.data.DataLoader(\n",
    "    dataset_train_simclr,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "\n",
    "dataset_test = LightlyDataset.from_torch_dataset(Subset(cifar10_train, classes_ids_train['dog']))\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Barlow Twins model\n",
    "\n",
    "Now we create the SimCLR model. We implement it as a PyTorch Lightning Module\n",
    "and use custom ResNet-20 backbone provided by Nikita Balabin. Lightly provides implementations\n",
    "of the Barlow Twins projection head and loss function in the `BarlowTwinsProjectionHead`\n",
    "and `BarlowTwinsLoss` classes. We can simply import them and combine the building\n",
    "blocks in the module. We will import constructed model from our `src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_135357/1655928042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbarlow_twins_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBarlowTwins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/PhD_study/ML/Final_project/model/barlow_twins_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBarlowTwinsProjectionHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_resnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_collate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightly'"
     ]
    }
   ],
   "source": [
    "from barlow_twins_model import BarlowTwins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check if a GPU is available and then train the module\n",
    "using the PyTorch Lightning Trainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "resnet_backbone = resnet20(num_classes=1)\n",
    "model = BarlowTwins(resnet_backbone, img_size = input_size)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50, gpus=gpus, progress_bar_refresh_rate=10\n",
    ")\n",
    "trainer.fit(model, dataloader_train_simclr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embeddings for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_embeddings\n",
    "\n",
    "model.eval()\n",
    "embeddings_test, filenames_test = generate_embeddings(model, dataloader_test)\n",
    "embeddings_train, filenames_train = generate_embeddings(model, dataloader_train_simclr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of TEST embeddings {embeddings_test.shape}')\n",
    "print(f'Shape of TRAIN embeddings {embeddings_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a helper function to generate embeddings\n",
    "from our test images using the model we just trained.\n",
    "Note that only the backbone is needed to generate embeddings,\n",
    "the projection head is only required for the training.\n",
    "Make sure to put the model into eval mode for this part!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Hausdorff distance between point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "hausdorff_dist = directed_hausdorff(embeddings_train, embeddings_test)[0]\n",
    "\n",
    "print(f'Hausdorff Dist: {hausdorff_dist:.3f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
